apiVersion: batch/v1
kind: Job
metadata:
  name: fine-tuning-gemma-queer-news
spec:
  backoffLimit: 0
  template:
    spec:
      containers:
        - name: fine-tuning-gemma-queer-news
          image: registry.datexis.com/s91000/thesis-tuning:0.0.9
          command: ["/bin/sh", "-c"]
          args: ["cd data && pip install accelerate tensorboardX sentencepiece protobuf && python code/main_no_class.py" ]
#          args: [ "cd data && pip install &&  python code/segment_articles.py" ]
          #          args: ["cd data && export CUDA_HOME=/usr/local/cuda && export PATH=\"${CUDA_HOME}/bin:$PATH\" && python code/main_no_class.py"]

          volumeMounts:
            - name: general-data
              mountPath: /tuning/data
            - name: root-temp
              mountPath: /root
#            - name: ray-pvc
#              mountPath: /pvc
#            - name: ssh-key
#              mountPath: /root/ssh-key
          ports:
#            - containerPort: 12345 # Ray internal communication.
#            - containerPort: 12346 # Ray internal communication.
            - containerPort: 22
          env:
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: MY_CPU_REQUEST
              valueFrom:
                resourceFieldRef:
                  resource: requests.cpu
            - name: MODEL_NAME
              value: 'google/gemma-7b'
              #              'google/gemma-7b'
              #              'meta-llama/Meta-Llama-3-8B'
              #                'mistralai/Mistral-7B-v0.3'
              #            "openlm-research/open_llama_13b"
            - name: HF_TOKEN
              value: "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"
            - name: DATASET_PATH
              value: 'data/datasets/queer_news.hf'
            - name: MODE
              value: "fine-tuning"
            - name: PROMPT_LENGTH
              value: "10"
            - name: OUTPUT_FILE
              value: "output.txt"
            - name: CHUNK_SIZE
              value: "10000"
            - name: LM_MODEL_PATH
              value: "google/gemma-7b"
            - name: REDUCE_DATASET
              value: "1"
            - name: REDUCE_DATASET_SIZE
              value: "10000"
            - name: BATCH_SIZE
              value: "2"
            - name: EPOCHS
              value: "1"
            - name: LEARNING_RATE
              value: "0.00002"

          resources:
            requests:
              nvidia.com/gpu: 8
              cpu: 1
            limits:
              nvidia.com/gpu: 8
              cpu: 2
      imagePullSecrets:
        - name: private-registry-auth
      restartPolicy: Never
      nodeSelector:
        gpu: a100
      volumes:
        - name: general-data
          persistentVolumeClaim:
            claimName: general-data
#        - name: ssh-key
#          secret:
#            secretName: my-ssh-public-key
#            defaultMode: 256
        - name: root-temp
          persistentVolumeClaim:
            claimName: root-temp