apiVersion: batch/v1
kind: Job
metadata:
  name: lora-gemma-queer-news-no-eval
spec:
  backoffLimit: 0
  template:
    spec:
      containers:
        - name: lora-gemma-queer-news-no-eval
          image: registry.datexis.com/s91000/thesis-tuning:0.0.14
          command: ["/bin/sh", "-c"]
#          command: ["python -m torch.distributed.launch --nproc_per_node=4  CUDA_LAUNCH_BLOCKING=1 code/main_no_class.py"]
          args: ["cd data && pip install --upgrade transformers trl bitsandbytes && torchrun --nproc_per_node=4 --master_addr=$(MY_POD_IP) --master_port=29500 code/fine_tuning_alternatives.py" ]
          volumeMounts:
            - name: general-data
              mountPath: /tuning/data
            - name: root-temp
              mountPath: /root
#            - name: ray-pvc
#              mountPath: /pvc
#            - name: ssh-key
#              mountPath: /root/ssh-key
          ports:
#            - containerPort: 12345 # Ray internal communication.
#            - containerPort: 12346 # Ray internal communication.
            - containerPort: 22
          env:
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: MY_CPU_REQUEST
              valueFrom:
                resourceFieldRef:
                  resource: requests.cpu
            - name: MODEL_NAME
              value: 'google/gemma-7b'
              #              'google/gemma-7b'
              #              'meta-llama/Meta-Llama-3-8B'
              #                'mistralai/Mistral-7B-v0.3'
              #            "openlm-research/open_llama_13b"
            - name: HF_TOKEN
              value: ""
            - name: DATASET_PATH
              value: 'data/datasets/tokenized_gemma-7b_queer_news.hf'
            - name: MODE
              value: "lora"
            - name: PROMPT_LENGTH
              value: "10"
            - name: OUTPUT_FILE
              value: "output.txt"
            - name: CHUNK_SIZE
              value: "10000"
            - name: LM_MODEL_PATH
              value: "google/gemma-7b"
            - name: REDUCE_DATASET
              value: "0"
            - name: REDUCE_DATASET_SIZE
              value: "100000"
            - name: BATCH_SIZE
              value: "2"
            - name: EPOCHS
              value: "1"
            - name: LEARNING_RATE
              value: "0.00002"
            - name: NCCL_DEBUG
              value: INFO
            - name: NUM_GPU
              value: "4"
            - name: GRADIENT_ACCUMULATION_STEPS
              value: "32"
          resources:
            requests:
              nvidia.com/gpu: 4
              cpu: 1
            limits:
              nvidia.com/gpu: 4
      imagePullSecrets:
        - name: private-registry-auth
      restartPolicy: Never
      nodeSelector:
        gpu: a100
      volumes:
        - name: general-data
          persistentVolumeClaim:
            claimName: general-data
#        - name: ssh-key
#          secret:
#            secretName: my-ssh-public-key
#            defaultMode: 256
        - name: root-temp
          persistentVolumeClaim:
            claimName: root-temp