%%
%% Berliner Hochschule für Technik --  Abschlussarbeit
%%
%% Kapitel 1
%%
%%

\chapter{Introduction}
\textcolor{bhtBlue}{
\textbf{Beginning:} Present the context and motivation for exploring bias in AI, specifically focusing on sexual identity in generative models. Explain the relevance of studying biases in large language models and the choice of soft-prompt tuning as an intervention method. Clarify the scope by stating that the focus will be on the technical aspects of bias detection and mitigation, without delving into broader sociopolitical discussions.
\textbf{Middle Part:} Define the objectives, including identifying and analyzing biases related to sexual identity and preference in Mistral. Introduce the datasets to be used, like HolisticBias \citep{holistic} and WinoQueer \citep{winoqueer}, and mention any hypotheses formed regarding the presence and nature of bias in the model.
\textbf{Conclusion:} Provide an overview of the structure of the thesis, summarizing the approach to investigating the bias through soft-prompt tuning and the expected contributions to the field.}
\\

\begin{quote}
    \large\enquote{Feeding AI system on the world's beauty, ugliness, and cruelty, but expecting it to reflect only the beauty is a fantasy.}
    \par 
    \hspace*{\fill}\small\citet{ruha}
\end{quote}


In less than ten years, the human population was confronted with an unprecedented change. With the work of \citet{vaswani2023attention}, the paths of our society crossed for the first time with the emerging capabilities of so-called \acrfull{llms}. Over the years, not only did researchers in the field of computer science and \acrfull{nlp} gain interest, but they also had an ever-increasing influence on people outside the field. With \acrshort{llms} like ChatGPT \citep{chatgpt-pub}, the amount of people that can possibly be influenced by outputs of those models increased rapidly. But what if these \acrfull{ai} models don't produce neutral output? Or even worse, even discriminate certain social groups and reproduce harmful stereotyping, wrong representation or output derogatory/exclusionary language? 
Several studies \textcolor{bhtRed}{source here!} show that \acrshort{llms} inherit bias to varying degrees. While gender and race bias in particular have been researched extensively, many other forms of bias have not been researched as intensively. This seems problematic as there are many more marginalised groups and also intersectionality of people.


A mobilisation of hatred, agitation and \endquote{demonisation} against LGBTQIA+ members can be observed \citep{tagesschau}. In order to avoid reinforcing discrimination, it is important to counteract it. AI systems that have an impact on our society should not feed into it. It is therefore important to counteract this and develop AI models that do not further discriminate against marginalised groups. The responsibility for this also lies with researchers in the field of computer science and the development of \acrshort{ai} models or \acrshort{llms}.  

\section{Motivation} 
As the numbers of users of \acrshort{ai} and especially \acrshort{llms} increases, the amount of people influenced by them also increases. As \acrshort{ai} models reproduce output with probabilities that they learned from training data, a specific world-view will be reproduced by the models. With that, already marginalized and/or discriminated groups will face the danger of facing even more harm. 
As discrimination should not be further enriched by the outputs of \acrshort{llms}, investigation of bias and possibilities to mitigate them needs to done. Furthermore, academic research does not often investigate bias based on gender or sexual identity. 

As developers of such models inherit a responsibility to keep an eye on what their models output, it's important to reflect and investigate it. In addition needs re-training or fine-tuning a lot of resources (energy, precious metals and time), where sustainability isn't often the subject. Therefor a resource conserving approach, soft-prompt tuning will be applied as an alternative to fine-tuning. 


\section{Objective}
This thesis shows an approach to identify sexual and gender identity bias  in generative \acrshort{ai}, namely \acrshort{llms}, as well as an approach for it's mitigation with a time and cost efficient alternative to fine-tuning – soft-prompt tuning. 

It should be demonstrated, that bias is inherited in \acrshort{ai} models, evaluated by f. i. datasets that are also created from people of the LGBTQIA+ community. With that, bias will be reduced which should lead to less generated content containing degrading and or stereotyping world-views and language.

Different pre-trained \acrshort{llms} will be used and evaluated for bias as well as fine-tuned and soft-prompt tuned to compare the effects of the methods as well as comparing trending \acrshort{llms} created by different companies, like Meta \citep{llama3} and Google \citep{gemma7}. 

\section{Contribution}
This research makes significant contributions to the field of \acrfull{nlp} by exploring the application of soft-prompt tuning to mitigate bias related to sexual and gender identity. The field of sexual and gender identity as a bias target group is underrepresented in current research, and the utilization of soft-prompt tuning for this specific purpose is not researched extensively. Additionally the focus is on these, often marginalised, groups and the importance of reducing bias in large language models of these groups. 

